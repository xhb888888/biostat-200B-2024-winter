---
title: "hw2"
author: "Hanbei Xiong"
format: pdf
editor: visual
---

# Problem 1

![](images/WeChat065c203038c7e29ee155cea4a3a9dfe8.png)

**Answer:**

$SSR=\sum(\hat{Y}-\bar{Y})^2$

Since $(\bar{X},\bar{Y})$ must be on the estimated regression line.

$SSR=\sum(\hat{\beta_0}+\hat{\beta_1}X_i-\hat{\beta_0}-\hat{\beta_1}\bar{X})$

$SSR=\hat{\beta_1^2}\sum(X_i-\bar{X})^2$

$F=\frac{MSR}{MSE}=\frac{\frac{SSR}{1}}{\frac{SSE}{n-2}}=\frac{\hat{\beta_1^2}\sum(X_i-\bar{X})^2}{MSE}$

$SE(\hat{\beta_1})=\frac{\sqrt{MSE}}{\sqrt{\sum(X_i-\bar{X})^2}}$

Hence:

$F=\frac{\hat{\beta_1^2}}{SE(\hat{\beta_1})^2}=(\frac{\hat{\beta_1}}{SE(\hat{\beta_1})})^2=t^2$

# Problem 2

![](images/WeChatcbdaa452ae5d71d7c3dbd5b3de1dbe83.png)

Note:

$\hat{\beta_0}=\bar{Y}-\hat{\beta_1}\bar{X}$

$\hat{\beta_1}=\frac{\sum(X_i-\bar{X})(Y_i-\bar{Y})}{\sum(X_i-\bar{X})^2}$

$\hat{\sigma}^2=\frac{1}{n-2}\sum(Y_i-\bar{Y_i})^2$

$R^2=\frac{SSR}{SSTO}=\frac{\sum(\hat{Y_i}-\bar{Y})^2}{\sum(Y_i-\bar{Y})^2}$

$t=\frac{\hat{\beta_1}}{SE(\hat{\beta_1})}=\frac{\hat{\beta_1}\sqrt{\sum(X_i-\bar{X})^2}}{\hat{\sigma}}$

**(a) Answer:**

Given $x_i$ is replaced be $cx_i$,

$\hat{\beta_0^*}=\bar{Y}-\hat{\beta_1^*}(c\bar{X})=\hat{\beta_0}$

$\hat{\beta_1^*}=\frac{\sum(cX_i-c\bar{X})(Y_i-\bar{Y})}{\sum(cX_i-c\bar{X})^2}=\frac{c\sum(X_i-\bar{X})(Y_i-\bar{Y})}{c^2\sum(X_i-\bar{X})^2}=\frac{\sum(X_i-\bar{X})(Y_i-\bar{Y})}{c\sum(X_i-\bar{X})^2}=\frac{1}{c}\hat{\beta_1}$

$\hat{\sigma^*}^2=\frac{1}{n-2}\sum(Y_i-\bar{Y_i})^2=\hat{\sigma}^2$

$R^{*2}=\frac{\sum(\hat{Y_i^*}-\bar{Y})^2}{\sum(Y_i-\bar{Y})^2}=\frac{\sum(\hat{\beta_0^*}+\hat{\beta_1^*}cX_i-\bar{Y})^2}{\sum(Y_i-\bar{Y})^2}=\frac{\sum(\hat{\beta_0}+\frac{1}{c}\hat{\beta_1}cX_i-\bar{Y})^2}{\sum(Y_i-\bar{Y})^2}=R^{2}$

$t^*=\frac{\hat{\beta_1^*}\sqrt{\sum(cX_i-c\bar{X})^2}}{\hat{\sigma^*}}=\frac{\frac{1}{c}\hat{\beta_1}\sqrt{c^2\sum(X_i-\bar{X})^2}}{\hat{\sigma}}=\frac{\hat{\beta_1}\sqrt{\sum(X_i-\bar{X})^2}}{\hat{\sigma}}=t$

**(b) Answer:**

Given $x_i$ is replaced be $x_i+d$,

$\hat{\beta_0^*}=\bar{Y}-\hat{\beta_1^*}(\bar{X}+d)=\bar{Y}-\hat{\beta_1}(\bar{X}+d)=\bar{Y}-\hat{\beta_1}\bar{X}-\hat{\beta_1}d=\hat{\beta_0}-\hat{\beta_1}d$

$\hat{\beta_1^*}=\frac{\sum((X_i+d)-(\bar{X}+d))(Y_i-\bar{Y})}{\sum((X_i+d)-(\bar{X}+d))^2}=\frac{\sum(X_i-\bar{X})(Y_i-\bar{Y})}{\sum(X_i-\bar{X})^2}=\hat{\beta_1}$

$\hat{\sigma^*}^2=\frac{1}{n-2}\sum(Y_i-\bar{Y_i})^2=\hat{\sigma}^2$

$R^{*2}=\frac{\sum(\hat{Y_i^*}-\bar{Y})^2}{\sum(Y_i-\bar{Y})^2}=\frac{\sum(\hat{\beta_0^*}+\hat{\beta_1^*}(X_i+d)-\bar{Y})^2}{\sum(Y_i-\bar{Y})^2}=\frac{\sum(\hat{\beta_0^*}+\hat{\beta_1^*}X_i+\hat{\beta_1^*}d-\bar{Y})^2}{\sum(Y_i-\bar{Y})^2}=\frac{\sum(\hat{\beta_0}-\hat{\beta_1}d+\hat{\beta_1}X_i+\hat{\beta_1}d-\bar{Y})^2}{\sum(Y_i-\bar{Y})^2}=R^{2}$

$t^*=\frac{\hat{\beta_1^*}\sqrt{\sum((X_i+d)-(\bar{X}+d))^2}}{\hat{\sigma^*}}=\frac{\hat{\beta_1}\sqrt{\sum(X_i-\bar{X})^2}}{\hat{\sigma}}=t$

**(c) Answer:**

Given $y_i$ is replaced be $ky_i$,

$\hat{\beta_0^*}=k\bar{Y}-\hat{\beta_1^*}\bar{X}=k\bar{Y}-k\hat{\beta_1}\bar{X}=k(\bar{Y}-\hat{\beta_1}\bar{X})=k\hat{\beta_0}$

$\hat{\beta_1^*}=\frac{\sum(X_i-\bar{X})(kY_i-k\bar{Y})}{\sum(X_i-\bar{X})^2}=\frac{k\sum(X_i-\bar{X})(Y_i-\bar{Y})}{\sum(X_i-\bar{X})^2}=k\hat{\beta_1}$

$\hat{\sigma^*}^2=\frac{1}{n-2}\sum(kY_i-k\bar{Y_i})^2=\frac{k^2}{n-2}\sum(Y_i-\bar{Y_i})^2=k^2\hat{\sigma}^2$

$R^{*2}=\frac{\sum(\hat{Y_i^*}-k\bar{Y})^2}{\sum(kY_i-k\bar{Y})^2}=\frac{\sum(\hat{\beta_0^*}+\hat{\beta_1^*}X_i-k\bar{Y})^2}{\sum(kY_i-k\bar{Y})^2}=\frac{\sum(k\hat{\beta_0}+k\hat{\beta_1}X_i-k\bar{Y})^2}{\sum(kY_i-k\bar{Y})^2}=\frac{k^2\sum(\hat{\beta_0}+\hat{\beta_1}X_i-\bar{Y})^2}{k^2\sum(Y_i-\bar{Y})^2}=R^2$

$t^*=\frac{\hat{\beta_1^*}\sqrt{\sum(X_i-\bar{X})^2}}{\hat{\sigma^*}}=\frac{k\hat{\beta_1}\sqrt{\sum(X_i-\bar{X})^2}}{k\hat{\sigma}}=\frac{\hat{\beta_1}\sqrt{\sum(X_i-\bar{X})^2}}{\hat{\sigma}}=t$

**(d) Answer:**

Given $y_i$ is replaced be $y_i+d$,

$\hat{\beta_0^*}=\bar{Y}+d-\hat{\beta_1^*}\bar{X}=\bar{Y}+d-\hat{\beta_1}\bar{X}=\hat{\beta_0}+d$

$\hat{\beta_1^*}=\frac{\sum(X_i-\bar{X})(Y_i+d-\bar{Y}-d)}{\sum(X_i-\bar{X})^2}=\frac{\sum(X_i-\bar{X})(Y_i-\bar{Y})}{\sum(X_i-\bar{X})^2}=\hat{\beta_1}$

$\hat{\sigma^*}^2=\frac{1}{n-2}\sum(Y_i+d-\bar{Y_i}-d)^2=\frac{1}{n-2}\sum(Y_i-\bar{Y_i})^2=\hat{\sigma}^2$

$R^{*2}=\frac{\sum(\hat{Y_i^*}-\bar{Y}-d)^2}{\sum(Y_i+d-\bar{Y_i}-d)^2}=\frac{\sum(\hat{\beta_0^*}+\hat{\beta_1^*}X_i-\bar{Y}-d)^2}{\sum(Y_i-\bar{Y})^2}=\frac{\sum(\hat{\beta_0}+d+\hat{\beta_1}X_i-\bar{Y}-d)^2}{\sum(Y_i-\bar{Y})^2}=\frac{\sum(\hat{Y_i}-\bar{Y})^2}{\sum(Y_i-\bar{Y})^2}=R^2$

$t^*=\frac{\hat{\beta_1^*}\sqrt{\sum(Y_i+d-\bar{Y_i}-d)^2}}{\hat{\sigma^*}}=\frac{\hat{\beta_1}\sqrt{\sum(Y_i-\bar{Y})^2}}{\hat{\sigma}}=t$

**(e) Answer:**

I will list the associated quantities below in order ($\hat{\beta_0},\hat{\beta_1},\hat{\sigma}^2,R^2,t$)

Original scaling of variables: 75.92256, 0.53609, 19.372, 0.858, 20.42

Convert age in month to age in year: 75.92256, 6.43307, 19.372, 0.858, 20.42

Subtract mean age: 104.61972, 0.53609, 19.372, 0.858, 20.42

Convert height in cm to height in inches: 29.89077, 0.21106, 3.0027, 0.858, 20.42

Subtract mean height: -28.69716, 0.53609, 19.372, 0.858, 20.42

Each quantities after fitting model with transformation matches with my mathematical derivation in part a), b), c), d).

# Problem 3

![](images/WeChat3952ce67e4ef94d72d2391a272c0f795.png)

**Answer:**

Here are summary statistics generated in SAS:

![](images/WeChat8cc1a41f917226fdddeb69b1895ae3e2.png)

Here are histograms for each variable:

![](images/WeChata060745e77f93ba583534952e1ed98a0.png){width="255"}

![](images/WeChat966fd712658a8b6af1b7e0539fcbba9b.png){width="254"}

![](images/WeChata3a013e2681e45fa2a720717917489b7.png){width="256"}

![](images/WeChatcb3bf0ca858e00d9ee1fba142b726e92.png){width="259"}

By observing the summary statistics, the mean and median of each variable except "nurses" are close to each other, which indicates that the distribution of each variable is close to normal distribution. For variable "nurses", the mean is greater than the median which means there are right skewness in the distribution. By observing the histograms, the distribution of each variable except "nurses" is close to normal distribution. The distribution of "nurses" is right skewed. The histograms verify our interpretation of summary statistics.

Here is scatter plot and fitted loess curve for a) between Risk and nurses:

![](images/WeChat62882ad9688733f5cd57f5bd5368edb9.png){width="340"}

Here is the original residuals analysis to check the assumptions:

![](images/WeChatdd94068e2a3eb5741699a0c376669d17.png){width="339"}

We can see residuals are more scattered in right of predicted value and more narrow to the right. The constancy of error does not seem to meet. The linearity assumption is closely to meet. Residuals seem to distribute normally.

Since the loess curve is relatively monotone and simple from fitted loess curve, we apply squared root transformation on x. Here is the updated residual analysis:

![](images/WeChatc774a850fb00fbb1941d563708d61b9c.png){width="342"}

The constancy of error seems to be verified after transformation. The linearity is a little off on both end side but show linearity in overall pattern. The distribution of residuals are a little right skewed but show normal distribution roughly.

Here is scatter plot and fitted loess curve for b) between Risk and length:

![](images/WeChatefcd1b33ccdb3c312a31737c42e115f2.png){width="361"}

Here is the original residuals analysis to check the assumptions:

![](images/WeChat2053fc89b8a1e6d1195b87d4d779645e.png){width="368"}

The constancy of error seems to meet as we ignore the outliar. The linearity assumption is closely to meet. Residuals seem to distribute normally. Therefore, we can apply linear regression model on this data.

Here is scatter plot and fitted loess curve for c) between nurses and svcs:

![](images/WeChata913a4602c2d050ca512163ed0d43a1f.png){width="377"}

Here is the original residuals analysis to check the assumptions:

![](images/WeChat098aef49c4421e20448928be065a873b.png){width="374"}

The assumption of constant error is obviously violated. The linearity assumption is closely to meet. Residuals show normal distribution. We apply log transformation on y. Here is the updated residual analysis:

![](images/WeChatf026286a8104e363ccd30a4d567fefcf.png){width="379"}

All assumptions seem to meet after this transformation.

# Problem 4

![](images/WeChatb2e8b640c9aea08a816ff2005aacd55c.png)

**(a) Answer:**

$\frac{\sum(X_i-\bar{X})(Y_i-\bar{Y})}{\sum(X_i-\bar{X})^2}=\frac{\sum(X_iY_i-X_i\bar{Y}-\bar{X}Y_i+\bar{X}\bar{Y})}{\sum(X_i-\bar{X})^2}$

$\frac{\sum(X_i-\bar{X})(Y_i-\bar{Y})}{\sum(X_i-\bar{X})^2}=\frac{\sum(X_iY_i-X_i\bar{Y}-\bar{X}Y_i)+n\frac{\sum{X_i}}{n}\bar{Y}}{\sum(X_i-\bar{X})^2}$

$\frac{\sum(X_i-\bar{X})(Y_i-\bar{Y})}{\sum(X_i-\bar{X})^2}=\frac{\sum(X_iY_i-X_i\bar{Y}-\bar{X}Y_i+X_i\bar{Y})}{\sum(X_i-\bar{X})^2}$

$\frac{\sum(X_i-\bar{X})(Y_i-\bar{Y})}{\sum(X_i-\bar{X})^2}=\frac{\sum(X_iY_i-\bar{X}Y_i)}{\sum(X_i-\bar{X})^2}$

$\frac{\sum(X_i-\bar{X})(Y_i-\bar{Y})}{\sum(X_i-\bar{X})^2}=\frac{\sum(X_i-\bar{X})Y_i}{\sum(X_i-\bar{X})^2}$

**(b) Answer:**

Let $c_i=\frac{X_i-\bar{X}}{\sum_{j=1}^{n}(X_j-\bar{X})^2}$

$Cov(\bar{Y},\hat{\beta_1})=Cov(\frac{1}{n}\sum{Y_i},\sum{c_iY_i})=\frac{1}{n}Cov(\sum{Y_i},\sum{c_iY_i})=\frac{1}{n}Cov(Y_1+Y_2+\dots+Y_n,c_1Y_1+c_2Y_2+\dots+c_nY_n)$

Since $Y_i$ are independent, $Cov(Y_i,Y_j)=0$ for $i\neq j$

$Cov(\bar{Y},\hat{\beta_1})=\frac{1}{n}(Cov(Y_1,c_1Y_1)+Cov(Y_2,c_2Y_2)+\dots+Cov(Y_n,c_nY_n))=\frac{1}{n}(\sum{c_iVar(Y_i)})=\frac{\sigma^2}{n}\sum{c_i}$

Since $\sum{c_i}=\frac{\sum(X_i-\bar{X})}{\sum_{j=1}^{n}(X_j-\bar{X})^2}=0$

$Cov(\bar{Y},\hat{\beta_1})=0$
